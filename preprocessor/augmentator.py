import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os
import re
from tqdm import tqdm  # Progress bar

def augment_reviews(input_path, augmentation_factor=2, batch_size=16):
    """
    Augments Korean review data using 'beomi/gemma-ko-7b' model.
    New data consists of an empty 'input' column, a newly generated 'output' column, and a sequential 'ID' column.
    
    :param input_path: Path to the original CSV file containing review data.
    :param augmentation_factor: Number of new similar reviews to generate per existing review (default: 2).
    :param batch_size: Number of reviews to process per batch (default: 16).
    """

    # Ensure CUDA is available
    if not torch.cuda.is_available():
        raise RuntimeError("ğŸš¨ CUDA is required. Please run on a GPU-enabled device.")

    print("âœ… CUDA detected: Running on GPU.")

    # Load model and tokenizer
    model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” + Flash Attention 2)
    model = AutoModelForCausalLM.from_pretrained(
        model_name
    )
    model.to("cuda")


    # Load CSV file
    try:
        df = pd.read_csv(input_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(input_path, encoding="cp949")

    # Ensure required columns exist
    if "output" not in df.columns or "ID" not in df.columns:
        raise ValueError("ğŸš¨ CSV file must contain 'output' and 'ID' columns.")

    augmented_data = []

    # Extract numerical part of existing IDs
    df["ID"] = df["ID"].astype(str)
    df["ID_num"] = df["ID"].apply(lambda x: int(re.search(r'\d+', x).group()) if re.search(r'\d+', x) else -1)
    
    # Find the last ID value
    max_id = df["ID_num"].max() if not df["ID_num"].isnull().all() else -1

    # Prepare the list of reviews to augment
    review_list = df["output"].tolist()

    # Process in batches
    for i in tqdm(range(0, len(review_list), batch_size), desc="ğŸ”„ Processing batches"):
        batch_texts = review_list[i : i + batch_size]  # Get batch
        batch_prompts = [
            f"""ìˆ™ë°•ì‹œì„¤ ë¦¬ë·°ì…ë‹ˆë‹¤. {review}
            """
            for review in batch_texts
            ]

        for _ in range(augmentation_factor):  # Generate multiple outputs per review
            inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True).to("cuda")

            with torch.no_grad():
                outputs = model.generate(
                    **inputs, max_new_tokens = 256, do_sample = True, temperature = 0.7
                    , top_p = 0.9
                    )

            # Store generated results
            for j, output in enumerate(outputs):
                # ê¸°ì¡´ ì½”ë“œì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ "ê·¸ë¦¬ê³ :"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ìˆì—ˆìŒ:
                # generated_review = tokenizer.decode(output, skip_special_tokens=True).split("ê·¸ë¦¬ê³ :")[-1].strip()

                # í”„ë¡¬í”„íŠ¸(ì…ë ¥ ë¦¬ë·°) ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ê³  ì‹¶ë‹¤ë©´, í”„ë¡¬í”„íŠ¸ì˜ ê¸¸ì´ì— ë§ì¶° ì˜ë¼ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                decoded_output = tokenizer.decode(output, skip_special_tokens=True)
                prompt_text = batch_prompts[j]
                # ë§Œì•½ ìƒì„± ê²°ê³¼ê°€ í”„ë¡¬í”„íŠ¸ë¡œ ì‹œì‘í•œë‹¤ë©´, í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì˜ë¼ëƒ…ë‹ˆë‹¤.
                if decoded_output.startswith(prompt_text):
                    generated_review = decoded_output[len(prompt_text):].strip()
                else:
                    generated_review = decoded_output.strip()

                # Generate new ID
                max_id += 1
                new_id = f"TRAIN_{max_id:05d}"

                augmented_data.append({ "ID": new_id, "input": "", "output": generated_review})

    # Convert results to DataFrame
    augmented_df = pd.DataFrame(augmented_data)

    # Combine with the original dataset
    final_df = pd.concat([df.drop(columns=["ID_num"]), augmented_df], ignore_index=True)

    # Generate new filename
    folder_path = os.path.dirname(input_path)
    base_filename = os.path.basename(input_path)

    # Modify filename: "train.csv" â†’ "augmented_train.csv"
    if base_filename.endswith(".csv"):
        file_name, ext = os.path.splitext(base_filename)
        new_filename = f"augmented_{file_name}.csv"
    else:
        new_filename = f"augmented_{base_filename}"

    output_path = os.path.join(folder_path, new_filename)

    # Save final dataset
    final_df.to_csv(output_path, index=False, encoding="utf-8-sig")
    
    print(f"âœ… Review data augmentation complete! Saved to: {output_path}")
