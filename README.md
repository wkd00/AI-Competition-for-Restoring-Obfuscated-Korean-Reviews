# NKTL-Noised-Korean-Translator-by-LSTM

### **LSTM을 활용한 난독화된 한글 리뷰 복원**  
 **윤희찬, 장덕재, 조태연**  

---

## 프로젝트 개요
**기존 형태소 분석기 및 생성 모델이 난독화된 한글 리뷰를 복원하는 데 한계를 보임.**  
- **트랜스포머 기반 모델(예: Llama3, DeepSeek)의 한계**  
- **기존 형태소 분석기의 OOV(Out-of-Vocabulary) 문제**  
- **문제 해결을 위해 LSTM과 커스텀 토크나이저 도입**  

**LSTM이 트랜스포머보다 효율적인 이유**  
- **적은 연산량** → **훈련 시간 감소**  
- **음절 단위 변형 복원 능력 우수**  
- **문맥을 길게 유지할 필요가 없어 복잡한 Attention 구조 불필요**  

**결과적으로, 트랜스포머 기반 모델보다 LSTM이 더 높은 성능을 보였음.**  

---

## 기존 방법론 분석

### 1. 기존 형태소 분석기의 문제점
- **형태소 분석기(Okt, Mecab, Kiwi 등)**는 난독화된 텍스트를 제대로 분해하지 못함.  
- 고유명사, 구어체 및 비표준 표현 처리가 어려움.  

 **예시)**  
**일반 문장**: `주변이 조용해서 좋았고 이 정도 상태면 정말 훌륭했습니다.`  
**형태소 분석 결과**: 
['주변', '이', '조용', '하', '어서', '좋', '았', '고', '이', '정도', '상태', '이', '면', '정말', '훌륭', '하', '었', '습니다', '.']

**난독화된 문장**: `죽펴닛 쪼옹헤셔 좋앝교 임 쩡또 샹탬먼 졍말 휼룽햇숲뉘댜.`  
**형태소 분석 결과**:  
['죽펴닛', '쪼옹헤셔', '좋앝교', '임', '쩡또', '샹탬먼', '졍말', '휼룽햇숲뉘댜', '.']

**일반 문장은 분해가 가능하지만, 난독화된 문장은 OOV 문제로 분석 불가능!**  

---

### 2️. 트랜스포머 기반 모델(LLM) 적용 시 한계
**초기 접근 방식**: Llama3, DeepSeek 등 트랜스포머 기반 LLM 적용  
**기대**: 문맥을 이해하고 자연스러운 문장 생성  
**문제점**:
- **노이징을 해석하는 능력이 부족** → 문자 수준 변형에 취약  
- ex) `"쫏쉭닿깝쥐"` → `"조식당까지"` 같은 변형을 정확히 복원하기 어려움  
- **과도한 연산 비용** → 모델 추론 속도가 느림  
- **Query Counting 문제** → Transformer는 특정 토큰의 등장 횟수를 정확히 추적하기 어려움
  ([참고 논문](https://arxiv.org/abs/2407.15160))  

**결론**: **LSTM을 사용하여 더 효율적인 솔루션을 찾자!**  

---

## 해결 방법

### **커스텀 토크나이저 적용**
- 기존 형태소 분석기는 **형태소 단위 분석 → 난독화된 문장 복원 불가**  
- 데이터 특성: **입출력 음절 수 동일, 글자 위치 동일**  
- 해결책: **음절 단위 & Noise-aware Tokenizer 도입!**  

### **LSTM 기반 모델 적용**
**왜 LSTM을 선택했는가?**
**Transformer보다 계산량이 현저히 적음** → 훈련 시간 감소  
**음절 시퀀스 학습 능력 우수** → 글자 수, 반복 패턴 복원에 강함  
**문맥을 길게 유지할 필요가 적음** → 복잡한 Attention 구조 불필요  

**LSTM 기반 모델이 최종적으로 생성 모델보다 높은 성능을 보였음!**  

**모델 구조 시각화 이미지**  
![image](https://github.com/user-attachments/assets/340d9a47-6efb-4380-b564-08481a382133)

---

## 실험 결과

**LSTM vs. 트랜스포머 기반 모델 파라미터 비교**  
| 모델 | 파라미터 수 |
|------|------------|
| **LSTM** | 25M |
| **DeepSeek-R1-Distill-Qwen-1.5B** | 1.5B |
| **Llama 3 8B** | 8B |
| **Gemma 2B** | 2B |

**LSTM은 최소 80배~최대 320배 적은 파라미터를 사용하면서도 높은 성능을 보임.**  
**경량 모델 + 데이터 특화 기법 → 최적 성능 달성!**  

---

## 최적화 과정 & 추가 실험

### 1️. **BiLSTM 도입** → 좌우 문맥 활용  
### 2️. **Pre-trained Word2Vec 임베딩 활용**  
### 3️. **Layer Normalization & Dropout 적용**  
### 4️. **데이터 증강 기법 적용**  
- **난독화된 문장과 원본 문장을 50% 비율로 섞어서 학습**

## 최종 결과

**[입력]** 윌뱐 잎츔민든릿 샤있샤윔엡 위썬 호뗄첨렴 관뤽갉 찰 앉 뙨는 누뀜뮈넬오.  
**[예측]** 일반 입주민들이 사이사이에 있어 호텔처럼 관리가 잘 안 되는 느낌이네요.  

**[입력]** 쾅얀땜고카 잚 뵤없 쑥쇼엔썬 편난학께 쀼룰 캄쌍할 쑤 위써셔 좋앝숩니탸.  
**[예측]** 광안대교가 잘 보여 숙소에서 편안하게 뷰를 감상할 수 있어서 좋았습니다. 

## 결론

**과도한 컴퓨팅 자원 대신 데이터 특성에 최적화된 솔루션 적용**  
**LSTM + 음절 단위 토크나이저가 문제 해결에 최적**  
**경량 모델 + 데이터 특화 기법으로 최적 성능을 달성!**  
**단순히 크고 좋은 모델(LLM)만이 정답이 아닐 수 있음**  

## 데이콘 수상 인터뷰
**축하합니다, hyican, Arirang, 응통이야 님! 수상의 영광을 함께 나누게 되어 기쁩니다.**

**우승의 기쁨을 맛본 소감을 한마디로 표현해 주세요.**
노력이 결실을 맺은 거 같아 기쁩니다. 

**팀의 이야기를 들려주세요.**
처음에 deepseek 등의 모델을 활용하다가 성능이 안 좋아서 포기할 뻔 했는데,
한 친구가 커스터마이징 토크나이저로 성능이 좋게 나온다는 점을 발견해 이를 통해 수상할 수 있었습니다

**여러분이 모인 계기, 팀명의 뜻, 혹은 개인 참가자로서의 여정 등을 자유롭게 나눠주세요.
여러분을 돋보이게 한 특별한 점은 무엇인가요?**
아무래도 커스텀 음절 토크나이저를 통한 경량화된 모델이 차별점이 아닐까 싶네요

**팀의 독특한 전략이나, 개인의 강점 등을 공유해 주세요.
이번 성과의 비결은 무엇이라고 생각하시나요?**
계속해서 모델을 개선하려한 점이 비결인 것 같습니다.

**대회 기간 중 특별히 기억에 남는 순간이 있다면 공유해 주세요.**
잠깐이나마 리더보드 1등을 했을 때가 기억에 남네요 아마 한 3일천하 정도 된 것 같습니다.

**여러분만의 노하우나 루틴을 공개해 주세요.
수상을 기념하여 빌 수 있는 한 가지 소원이 있다면?**
다음엔 상금이 걸린 대회를 우승해보고 싶습니다 ㅎ

**앞으로의 목표와 꿈을 말씀해 주세요.**
더 많은 경험을 쌓아 능력있는 데이터 엔지니어와 같은 각 분야의 전문가가 되고 싶습니다.
